import pickle
import tensorflow as tf
import numpy as np
from keras.layers import Input, Flatten, Dense
from keras.models import Model
import keras
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam , SGD
from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix
# Import to_categorical from the keras utils package to one hot encode the labels
from keras.utils import to_categorical      


classes = 10

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()


X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify = y_train)

# Shape before one hot encoding
print("Shape before one hot encoding")
print(X_train.shape, y_train.shape)
print(X_valid.shape, y_valid.shape)
print((X_test.shape,y_test.shape))
print("")

# convert from integers to floats
train_norm = X_train.astype('float32')
test_norm = X_test.astype('float32')
Valid_norm = X_valid.astype('float32')


# normalize to range 0-1
x_train_norm = train_norm / 255.0
x_test_norm = test_norm / 255.0
x_Valid_norm = Valid_norm / 255.0



#Converting the labels into one hot encoding
y_train = keras.utils.to_categorical(y_train, classes)
y_valid = keras.utils.to_categorical(y_valid, classes)
y_test = keras.utils.to_categorical(y_test, classes)



# Shape before one hot encoding
print("Shape before one hot encoding")
print(X_train.shape, y_train.shape)
print(X_valid.shape, y_valid.shape)
print((X_test.shape,y_test.shape))
print("")





VGG_pretrained_model = tf.keras.applications.vgg16.VGG16(
                include_top= False,             # to use your own input and output layers
                weights="imagenet",             # Load pretrained weights
                input_shape= (32,32,3),         # Input image shape
                pooling='avg',                  # average pooling / max pooling
                classes=10,                     # Number of classes
            )



for layer in VGG_pretrained_model.layers:
    layer.trainable = True


new_vgg_model = tf.keras.Sequential()

new_vgg_model.add(tf.keras.layers.Flatten())
new_vgg_model.add(tf.keras.layers.Dense(128,activation='relu'))
new_vgg_model.add(tf.keras.layers.Dense(64,activation='relu'))
new_vgg_model.add(tf.keras.layers.Dense(32,activation='relu'))
new_vgg_model.add(tf.keras.layers.Dropout(0.2))
new_vgg_model.add(tf.keras.layers.Dense(10, activation='softmax'))


new_vgg_model.compile(optimizer= SGD(learning_rate=0.05),loss='categorical_crossentropy',metrics='accuracy')


history = new_vgg_model.fit(x_train_norm, y_train, batch_size=32, epochs=12,validation_data=(x_Valid_norm, y_valid))


new_vgg_model.summary()

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()